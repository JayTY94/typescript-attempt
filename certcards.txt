





EMPTY
1
UiPath Document Understanding
The other outcome of the digitization process is all the text from the document, stored in a string variable.





2
UiPath Document Understanding
Classification is done through the Classify Document Scope and it's performed by classifiers.

Basically, the document text and object model resulted in the digitization steps are sent to the Classifiers, which report what types they recognize within the incoming file.





3
UiPath Document Understanding
What happens if a document is not classified correctly...

  1. If training step is implemented, the classifier will get more accurate over time.


4
UiPath Document Understanding
What happens if a document is not classified correctly...

  2. The robot tries to extract values for the fields defined in the Taxonomy for classification made



5
UiPath Document Understanding
What happens if a document is not classified correctly...

  3. You can correct the classification in the Validation Station.



6
UiPath Document Understanding
Why do you need to map the fields in hte Extractor configuration when using the MS Extractor?

  Some extractors have their own Taxonomy, which is why you need to map the fields - this is the case for the ML Extractor.



7
UiPath Document Understanding
Take this scenario: you need to extract data from medical records, but all the documents come in a single PDF file.  Based on a list of classification results, you loop through each and perform data extraction.

What arguments do you add in the data extraction scope? Select all that apply.
  Taxaonomy
  Document path
  Document Object Model
  Document Text
  Validated Classification Result


8
UiPath Document Understanding
Select all of the extractors you would use to extract data from a semi-structured document.

  The Regex extractor
  The Machine Learning Extractor
  NOT
  The Form Extractor
  The Intelligent Form Extractor



9
UiPath Document Understanding
Select all the actions that you can perform inside the Validation Station
  Change extracted value
  Add an additional value
  Add rows and cell values to a Table
  Correct classification results





10
UiPath Document Understanding
What are the mandatory steps for processing a document with the intelligent OCR activities.
  Digitize Document
  Create the Taxonomy
  Loading the Taxonomy
  Data Extraction with at least one Extractor



11
UiPath Document Understanding
[true] You can extract data from rotated or skewed documents.



12
UiPath Document Understanding
The Configure Classifiers Wizard accessed via the Train Classifiers Scope allows you to choose which classifiers are trained for each document type.




13
.NET
A DataTable is an in-memory representation of a single database table which has collection of rows and columns where as a DataSet is an in-memory representation of a database-like structure which has a collection of DataTables.



14
UiPath Document Understanding
The Train Classifiers Scope empowers the closing of the feedback loop to any classification algorithm capable of learning. Drag and drop your classifier trainers within this Scope activity and enable them using the Configure Classifiers wizard to ensure that the information validated by humans through the Classification Station or Validation Station is used by your classifiers to improve their own performance.





15
UiPath Document Understanding
The Machine Learning Extractor leverages the power of AI and Machine Learning to identify information in structured or semi-structured documents by either using one of  UiPath's public data extraction services or by calling custom trained Machine Learning models that you can build and host in AI Fabric. This activity is part of the UiPath.DocumentUnderstanding.ML.Activities package.



16
UiPath Document Understanding
You can also enable human validation through Long-Running Workflows, optimizing human-robot collaboration. The Create Document Validation Action and the Wait for Document Validation Action and Resume activities enable this scenario.



17
Asana
Inbox is a filtered news feed of updates, but only for the work you're following. You can also respond to things right from inbox to quickly navigate to your work.



18
Asana
Organizations are based on your company's shared email domain and connect everybody within your company that uses Asana.



19
Asana
Teams are functional groups in an Organization that likely correspond to general teams, like marketing or sales, or functional groups like "NYC Office".



20
Asana
Projects are stored in teams (and Portfolios) to track all actionable steps, information, and communications towards achieving a goal, initiative, or objective.




21
Asana
Tasks are stored in projects and make it clear who's responsible for what by when. Tasks store all files, messages, and instructions related to it so they're easy to find in one place.




22
Asana
Portfolios store groups of projects. You can use them to see all the projects and status updates towards an initiative or objective in one place.




23
Asana
Tasks matched by project, task, or subtask

Editorial Calendar => Create a project
Publish weekly blog post => Create a task
Gather customer qutoes for blog => Create a subtask




C:\Users\m4ple\PycharmProjects\pythonProject1\venv\Scripts\python.exe C:/Users/m4ple/PycharmProjects/pythonProject1/crads.py
24
Azure Data Fundamentals
Data structures in which this data is organized often represents entities that are important to an organization (such as customers, products, sales orders, and so on).



25
Azure Data Fundamentals
One common format for semi-structured data is JavaScript Object Notation (JSON). The example below shows a pair of JSON documents that represent customer information.




26
Azure Data Fundamentals
Structured data is data that adheres to a fixed schema, so all of the data has the same fields or properties. Most commonly, the schema for structured data entities is tabular - in other words, the data is represented in one or more tables that consist of rows to represent each instance of a data entity, and columns to represent attributes of the entity.



27
Azure Data Fundamentals
n some cases, a key (primary or foreign) can be defined as a composite key based on a unique combination of multiple columns. For example, the LineItem table in the example above uses a unique combination of OrderNo and ItemNo to identify a line item from an individual order.



28
Azure Data Fundamentals
SQL statements are grouped into three main logical groups:

Data Definition Language (DDL)
Data Control Language (DCL)
Data Manipulation Language (DML)



29
Azure Data Fundamentals
The most common DDL statements are:

Statement	Description
CREATE	Create a new object in the database, such as a table or a view.
ALTER	Modify the structure of an object. For instance, altering a table to add a new column.
DROP	Remove an object from the database.
RENAME	Rename an existing object.



30
Azure Data Fundamentals
The three main DCL statements are:

Statement	Description
GRANT	Grant permission to perform specific actions
DENY	Deny permission to perform specific actions
REVOKE	Remove a previously granted permission




Process finished with exit code 0
C:\Users\m4ple\PycharmProjects\pythonProject1\venv\Scripts\python.exe C:/Users/m4ple/PycharmProjects/pythonProject1/crads.py
1
Azure Data Fundamentals
Delimited Text files
Data is often stored in plain text format with specific field delimiters and row terminators. The most common format for delimited data is comma-separated values (CSV) in which fields are separated by commas, and rows are terminated by a carriage return / new line. Optionally, the first line may include the field names.



2
Azure Data Fundamentals
Extensible Markup Language (XML)
XML is a human-readable data format that was popular in the 1990s and 2000s. It's largely been superseded by the less verbose JSON format, but there are still some systems that use XML to represent data. XML uses tags enclosed in angle-brackets (<../>) to define elements and attributes, as shown in this example:



3
Azure Data Fundamentals
Avro is a row-based format. It was created by Apache. Each record contains a header that describes the structure of the data in the record. This header is stored as JSON. The data is stored as binary information. An application uses the information in the header to parse the binary data and extract the fields it contains. Avro is a good format for compressing data and minimizing storage and network bandwidth requirements.





4
Azure Data Fundamentals
ORC (Optimized Row Columnar format) organizes data into columns rather than rows. It was developed by HortonWorks for optimizing read and write operations in Apache Hive (Hive is a data warehouse system that supports fast data summarization and querying over large datasets). An ORC file contains stripes of data. Each stripe holds the data for a column or set of columns. A stripe contains an index into the rows in the stripe, the data for each row, and a footer that holds statistical information (count, sum, max, min, and so on) for each column.





5
Azure Data Fundamentals
Parquet is another columnar data format. It was created by Cloudera and Twitter. A Parquet file contains row groups. Data for each column is stored together in the same row group. Each row group contains one or more chunks of data. A Parquet file includes metadata that describes the set of rows found in each chunk. An application can use this metadata to quickly locate the correct chunk for a given set of rows, and retrieve the data in the specified columns for these rows. Parquet specializes in storing and processing nested data types efficiently. It supports very efficient compression and encoding schemes.





6
Azure Data Fundamentals
OLTP; ACID
Atomicity – each transaction is treated as a single unit, which succeeds completely or fails completely. For example, a transaction that involved debiting funds from one account and crediting the same amount to another account must complete both actions. If either action can't be completed, then the other action must fail.



7
Azure Data Fundamentals
OLTP; ACID
Consistency – transactions can only take the data in the database from one valid state to another. To continue the debit and credit example above, the completed state of the transaction must reflect the transfer of funds from one account to the other.



8
Azure Data Fundamentals
OLTP; ACID
Isolation – concurrent transactions cannot interfere with one another, and must result in a consistent database state. For example, while the transaction to transfer funds from one account to another is in-process, another transaction that checks the balance of these accounts must return consistent results - the balance-checking transaction can't retrieve a value for one account that reflects the balance before the transfer, and a value for the other account that reflects the balance after the transfer.



9
Azure Data Fundamentals
OLTP; ACID
Durability – when a transaction has been committed, it will remain committed. After the account transfer transaction has completed, the revised account balances are persisted so that even if the database system were to be switched off, the committed transaction would be reflected when it is switched on again.



10
Azure Data Fundamentals
Data warehouses are an established way to store data in a relational schema that is optimized for read operations – primarily queries to support reporting and data visualization. The data warehouse schema may require some denormalization of data in an OLTP data source (introducing some duplication to make queries perform faster).





11
Azure Data Fundamentals
Azure Database for MySQL - a simple-to-use open-source database management system that is commonly used in Linux, Apache, MySQL, and PHP (LAMP) stack apps.





12
Azure Data Fundamentals
Azure Database for MariaDB - a newer database management system, created by the original developers of MySQL. The database engine has since been rewritten and optimized to improve performance. MariaDB offers compatibility with Oracle Database (another popular commercial database management system).





13
Azure Data Fundamentals
Azure Database for PostgreSQL - a hybrid relational-object database. You can store data in relational tables, but a PostgreSQL database also enables you to store custom data types, with their own non-relational properties.





14
Azure Data Fundamentals
Data engineers use Azure Storage to host data lakes - blob storage with a hierarchical namespace that enables files to be organized in folders in a distributed file system.



15
Azure Data Fundamentals
Azure Cosmos DB is a global-scale non-relational (NoSQL) database system that supports multiple application programming interfaces (APIs), enabling you to store and manage data as JSON documents, key-value pairs, column-families, and graphs.



16
Azure Data Fundamentals
Azure Data Factory is used by data engineers to build extract, transform, and load (ETL) solutions that populate analytical data stores with data from transactional systems across the organization.





17
Azure Data Fundamentals
Azure Synapse Analytics is a comprehensive, unified data analytics solution that provides a single service interface for multiple analytical capabilities, including:
    Pipelines
    SQL
    Apache Spark
    Azure Synapse Data Explorer



18
Azure Data Fundamentals
Azure Databricks is an Azure-integrated version of the popular Databricks platform, which combines the Apache Spark data processing platform with SQL database semantics and an integrated management interface to enable large-scale data analytics.





19
Azure Data Fundamentals
Data engineers can incorporate Azure Stream Analytics into data analytics architectures that capture streaming data for ingestion into an analytical data store or for real-time visualization.





20
Azure Data Fundamentals
Data analysts can use Azure Data Explorer to query and analyze data that includes a timestamp attribute, such as is typically found in log files and Internet-of-things (IoT) telemetry data.






21
Azure Data Fundamentals
Which single service would you use to implement data pipelines, SQL analytics, and Spark analytics?

Azure Synapse Analytics

NOT Microsoft Power BI
NOT Azure SQL Database



22
Azure Data Fundamentals
Normalization is a term used by database professionals for a schema design process that minimizes data duplication and enforces data integrity.





23
Azure Data Fundamentals
Principle of Normalization 1 of 4
Separate each entity into its own table



24
Azure Data Fundamentals
Principle of Normalization 2 of 4
Separate each discrete attribute



25
Azure Data Fundamentals
Principle of Normalization 3 of 4
Uniquely identify each entity instance (row) using a primary key.



26
Azure Data Fundamentals
Principle of Normalization 4 of 4
Use foreign key columns to link related entities.




27
Azure Data Fundamentals
The four main DML statements are:

Statement	Description
SELECT	Read rows from a table
INSERT	Insert new rows into a table
UPDATE	Modify data in existing rows
DELETE	Delete existing rows



28
Azure Data Fundamentals




29
Azure Data Fundamentals




30
Azure Data Fundamentals





Process finished with exit code 0
