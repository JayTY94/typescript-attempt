


Process finished with exit code 0
1
Azure Data Fundamentals
Pipelines consist of one or more activities that operate on data. An input dataset provides the source data, and activities can be defined as a data flow that incrementally manipulates the data until an output dataset is produced. Pipelines use linked services to load and process data â€“ enabling you to use the right technology for each step of the workflow


2
Azure Data Fundamentals
A fact and dimension table schema is called a star schema; though it's often extended into a snowflake schema by adding additional tables related to the dimension tables to represent dimensional hierarchies (for example, product might be related to product categories)


3
Azure Data Fundamentals
Technologies like Spark or Hadoop are often used to process queries on the stored files and return data for reporting and analytics. These systems often apply a schema-on-read approach to define tabular schemas on semi-structured data files at the point where the data is read for analysis, without applying constraints when it's stored


4
Azure Data Fundamentals
Databricks provides an interactive user interface through which the system can be managed and data can be explored in interactive notebooks.




5
Azure Data Fundamentals
Data processing is simply the conversion of raw data to meaningful information through a process. there are two general ways to process data:

  Batch processing, in which data records are collected and stored before being processed together in a single operation.


6
Azure Data Fundamentals
Data processing is simply the conversion of raw data to meaningful information through a process. there are two general ways to process data:

  Stream processing, in which a source of data is constantly monitored and processed in real time as new data events occur.


7
Azure Data Fundamentals
Advantages of batch processing include:

  Large volumes of data  can be processed at a convenient time.
  It can be scheduled to run at a time when computers or systems might otherwise be idle, such as overnight or during off-peak hours.


8
Azure Data Fundamentals
Disadvantages of batch processing include:

  The time delay between ingesting data and getting the results.

  All of a batch job's input data must be ready before a batch can be processed. This means data must be carefully checked. Problems with data, errors, and program crashes that occur during batch jobs bring the whole process to a halt.


9
Azure Data Fundamentals
In stream processing, each new piece of data is processed when it arrives. Unlike batch processing, there's no waiting until the next batch processing interval - data is processed as individual units in real-time rather than being processed a batch at a time.


10
Azure Data Fundamentals
Batch processing can process all the data in the dataset. Stream processing typically only has access to the most recent data received, or within a rolling time window (the last 30 seconds, for example).




11
Azure Data Fundamentals
Batch processing is suitable for handling large datasets efficiently. Stream processing is intended for individual records or micro batches consisting of few records.




12
Azure Data Fundamentals
Latency is the time taken for the data to be received and processed. The latency for batch processing is typically a few hours. Stream processing typically occurs immediately, with latency in the order of seconds or milliseconds.




13
Azure Data Fundamentals
You typically use batch processing to perform complex analytics. Stream processing is used for simple response functions, aggregates, or calculations such as rolling averages.




14
General
In computing, tar is a computer software utility for collecting many files into one archive file, often referred to as a tarball, for distribution or backup purposes. The name is derived from "tape archive", as it was originally developed to write data to sequential I/O devices with no file system of their own. 




15
General
When you download the source for libpri, DAHDI, and Asterisk you'll typically end up with files with a .tar.gz or .tgz file extension. These files are affectionately known as tarballs. The name comes from the tar Unix utility, which stands for tape archive. A tarball is a collection of other files combined into a single file for easy copying, and then often compressed with a utility such as GZip.


16
Azure Data Fundamentals
At its simplest, a high-level architecture for stream processing looks like this:
  1. An event generates some data
  2. The generated data is captured in a streaming source for processing
  3. The event is processed, often by a perpetual query.
  4. The results of the stream processsing operation are written to an output (or sink).



17
Azure Data Fundamentals
Microsoft Azure supports multiple technologies that you can use to implement real-time analytics of streaming data, including:

Azure Stream Analytics: A platform-as-a-service (PaaS) solution that you can use to define streaming jobs that ingest data from a streaming source, apply a perpetual query, and write the results to an output.


18
Azure Data Fundamentals
Microsoft Azure supports multiple technologies that you can use to implement real-time analytics of streaming data, including:

Azure Data Explorer: A high-performance database and analytics service that is optimized for ingesting and querying batch or streaming data with a time-series element, and which can be used as a standalone Azure service or as an Azure Synapse Data Explorer runtime in an Azure Synapse Analytics workspace.


19
Azure Data Fundamentals
The following services are commonly used to ingest data for stream processing on Azure:

  Azure Event Hubs: A data ingestion services that you can use to manage queues of event data, ensuring that each event is processed in order, exactly once.


20
Azure Data Fundamentals
The following services are commonly used to ingest data for stream processing on Azure:

  Apache Kafka: An open-source data ingestion solution that is commonly used together with Apache Spark. You can use Azure HDInsight to create a Kafka cluster.


21
Azure Data Fundamentals
The output from stream processing is often sent to the following services:

Azure Event Hubs
Azure Data Lake Store Gen 2 or Azure blob storage
Azure SQL Database or Azure Synapse Analytics, or Azure Databricks
Microsoft Power BI


22
Azure Data Fundamentals
Azure Stream Analytics is a great technology choice when you need to continually capture data from a streaming source, filter or aggregate it, and send the results to a data store or downstream process for analysis and reporting.



23
Azure Data Fundamentals
If your stream process requirements are complex or resource-intensive, you can create a Stream Analysis cluster, which uses the same underlying processing engine as a Stream Analytics job, but in a dedicated tenant (so your processing is not affected by other customers) and with configurable scalability that enables you to define the right balance of throughput and cost for your specific scenario.


24
REFramework Project with Tabular Data
Configuration steps:
  1. Ensure prerequisites are covered
  2. Create a new project and fill in the Config file
  3. CHange the TransactionItem datatype
  4. Configure the workflows used to open, close, and kill apps
  5. Edit the GetTransactionsData and Process workflows


25
REFramework Project with Tabular Data
When whiteboarding your process workflows, what information should you take into account?
  Workflow name
  Pre-condition
  Description
  Post-action
  Arguments
  State(s)


26
REFramework Project with Tabular Data
When working with tabular data, where would you set the MaxRetryNumber?
In the Constants sheet of the Config file

NOT In the Settings sheet of the Config file





27
REFramework Project with Tabular Data
By default, which states are affected by the TransactionItem datatype change?

  Get Transaction Data
  Process Transaction
  NOT Initialization
  NOT End Process


28
REFramework Project with Tabular Data
CloseAllApplications.xaml is used to log out and close the target applications

KillAllProcesses.xaml is used to force-close the target applications




29
REFramework Project with Tabular Data
If the TransactionData variable is of type DataTable, what type should the in_TransactionItem argument in Process.xaml have?
  DataRow
  NOT QueueItem
  NOT String
  NOT DataTable
  
30
REFramework Project with Tabular Data
We are planning to build an execution report functionality which includes the transaction identifier and the transaction status. What workflow would this logic best be included in?
  
  SetTransactionStatus.xaml 
  NOT GetTransactionData.xaml
  NOT RetryCurrentTransaction.xaml


1
REFramework Project with Tabular Data
[True]When working with REFramework template, you won't be able to create new test cases


2
REFramework Project with Tabular Data
You have just set the TransactionItem type to MailMessage. Which of the following types is a valid option for TransactionData?
List(of MailMessage)



3
REFramework Project with Tabular Data
You want to configure the REFramework template to build a linear process. Which argument do you need to monitor in GetTransactionData.xaml?
  in_TransactionNumber
  NOT in_Config
  NOT out_TransactionItem

4
REFramework Project with Tabular Data
What will be the type of TransactionItem if TransactionData is an Array of Strings?
  String
  NOT String[]



5
REFramework Project with Tabular Data
What action you need to take if you want to enable the MaxConsecutiveSystemExceptions feature in the REFramework template?
Set the MaxConsecutiveSystemExceptions constant value to greater than zero



6
REFramework Project with Tabular Data
Which argument in the Process workflow holds the business context information? 
In_TransactionItem


7
REFramework Project with Tabular Data
If we intend to use tabular data instead of an Orchestrator queue, which activity do we need to replace in the default REFramework template to retrieve the transaction data? 
The Get Transaction Item  activity in the Get Transaction Data state.





8
REFramework Project with Tabular Data
What is the default type of the 'TransactionData' variable?
DataTable
NOT Queue Item




9
REFramework Project with Tabular Data
The process is based on the REFramework template. Asset "Userdetails" was referenced in the config file, but it wasn't created in Orchestrator. What happens when you run the process?

  It throws an error in the Initialization state.




10
REFramework Project with Tabular Data
Is it possible to use tabular or other types of data for the transaction item and Orchestrator assets in the same process?

Yes. You can use Assets independently from Queues. 




11
REFramework Project with Tabular Data
From the given options, identify in which SetTransactionStatus workflow file is invoked.

  In the Try block of the process transaction state.
  Within the Business Rule Exception section in the catch section of process transaction.
  Within the System Exception section in the catch section of process transaction.






12
REFramework Project with Tabular Data
From the given option, identify the test cases available by default with REFramework template.
  GeneralTestCase
  GetTransactionDataTestCase
  MainTestCase
  ProcessTestCase
  InitAllApplicationsTestCase
  InitAllSettingsTestCase




13
Azure Data Fundamentals



14
Azure Data Fundamentals



15
Azure Data Fundamentals



16
Azure Data Fundamentals



17
Azure Data Fundamentals



18
Azure Data Fundamentals



19
Azure Data Fundamentals



20
Azure Data Fundamentals



21
Azure Data Fundamentals



22
Azure Data Fundamentals



23
Azure Data Fundamentals



24
Azure Data Fundamentals



25
Azure Data Fundamentals



26
Azure Data Fundamentals



27
Azure Data Fundamentals



28
Azure Data Fundamentals



29
Azure Data Fundamentals



30
Azure Data Fundamentals


