



6
UiPath Mail Activities
By default, the pack provides specific activities for Exchange, IBM Notes, IMAP, Outlook, POP3, and SMTP.



7
UiPath Mail Activities
The object type we use for retrieved emails is System.Net.Mail.MailMessage.



8
UiPath Mail Activities
When building a process which uses email, for most activity groups, some configuration may be required on the email account.



9
UiPath Mail Activities
The Get IMAP Mail Messages activity lets us filter messages based on different criteria.



10
UiPath Mail Activities
A great resource to learn more about filtering emails is the Microsoft documentation page.



11
UiPath Mail Activities
To create an email template, write the message in a new text document and then use a 'Read Text' activity to read it.

To store the template text, we must create a new variable in the 'output to' property field




12
Azure Data Fundamentals
An extract, transform, and load (ETL) process requires data that is fully processed before loading to the target data store.


13
Azure Data Fundamentals
Transcribing audio files is an example of cognitive analytics


14
Azure Data Fundamentals
A Clustered index is a type of index in which table records are physically reordered to match the index.	A Non-Clustered index is a special type of index in which logical order of index does not match physical stored order of the rows on disk.


15
Azure Data Fundamentals
Descriptive Analytics tell you what occurred in the past.


16
Azure Data Fundamentals
Relational databases are optimized for writes. They are optimized for consistency and availability. Advantages of relational databases include simplicity, ease of data retrieval, data integrity, and flexibility.



17
Azure Data Fundamentals
Batch processing can output data to a file store but not a relational database, nor a NoSQL database.


18
Azure Data Fundamentals
Your company plans to load data from a customer relationship management (CRM) system to a data warehouse by using an extract, load, and transform (ELT) process.
Where does data processing occur for each stage of the ELT process?

Extract: The CRM system
Load: The Data Warehouse
Transform: An in-memory data integration tool


19
Azure Data Fundamentals
Treemaps are charts of colored rectangles, with size representing value. They can be hierarchical, with rectangles nested within the main rectangles.

A key influencer chart displays the major contributors to a selected result or value.



20
Azure Data Fundamentals
Azure Storage offers two options for copying your data to a secondary region:
✑ Geo-redundant storage (GRS)
✑ Geo-zone-redundant storage (GZRS)
B: With GRS or GZRS, the data in the secondary region isn't available for read or write access unless there is a failover to the secondary region. For read access to the secondary region, configure your storage account to use read-access geo-redundant storage (RA-GRS) or read-access geo-zone-redundant storage (RA-GZRS).


21
Azure Data Fundamentals
[True] PaaS database offerings in Azure reuqire less setup and configuration than IaaS database offerings
[False] PaaS database offerings in Azure provide end users with the ability to control and update the OS version
[False] All relational and non-relational PaaS database offerings in Azure can be paused to reduce costs.


22
Azure Data Fundamentals
A key/value store associates each data value with a unique key. Most key/value stores only support simple query, insert, and delete operations. To modify a value (either partially or completely), an application must overwrite the existing data for the entire value. In most implementations, reading or writing a single value is an atomic operation.


23
Azure Data Fundamentals
Azure file shares are deployed into storage accounts, which are top-level objects that represent a shared pool of storage.
(In nesting order)
Azure Resource Group
  Azure Storage Account
    File Share
      Folders
        Files


24
Azure Data Fundamentals
In Azure Data Factory, a data factory might have one or more pipelines. A pipeline is a logical grouping of activities that performs a unit of work. Together, the activities in a pipeline perform a task. For example, a pipeline can contain a group of activities that ingests data from an Azure blob, and then runs a Hive query on an HDInsight cluster to partition the data.




25
Azure Data Fundamentals
Create and manage graphs of data transformation logic that you can use to transform any-sized data. You can build-up a reusable library of data transformation routines and execute those processes in a scaled-out manner from your ADF pipelines. Data Factory will execute your logic on a Spark cluster that spins-up and spins-down when you need it.


26
Azure Data Fundamentals
Activities represent a processing step in a pipeline. For example, you might use a copy activity to copy data from one data store to another data store. Azure Data Factory supports three types of activities: data movement activities, data transformation activities, and control activities.




27
Azure Data Fundamentals
In Azure Data Factory, datasets represent data structures within the data stores, which simply point to or reference the data you want to use in your activities as inputs or outputs.




28
Azure Data Fundamentals
Linked services are used for two purposes in Data Factory:

To represent a data store that includes, but isn't limited to, a SQL Server database, Oracle database, file share, or Azure blob storage account. For a list of supported data stores, see the copy activity article.




29
Azure Data Fundamentals
Linked services are used for two purposes in Data Factory:

To represent a compute resource that can host the execution of an activity. For example, the HDInsightHive activity runs on an HDInsight Hadoop cluster. For a list of transformation activities and supported compute environments, see the transform data article.


30
Azure Data Fundamentals
In Azure Data Factory, A pipeline run is an instance of the pipeline execution. Pipeline runs are typically instantiated by passing the arguments to the parameters that are defined in pipelines. The arguments can be passed manually or within the trigger definition.

1
General 
Workato is the RPA alternatvie recommended to us by the potential partner Neostella.


2
Workato 
Workato is the only integration platform that was built from the ground up to support a single design interface for developer/IT and citizen integrators, ensuring that citizens and IT have access to the same ease-of-use driven productivity improvements, and at the same time offering citizens the same power and capability available to IT.





3
Workato
Governance is a key requirement for any platform that allows business users to work with corporate data. Workato Aegis is a management tool that provides cross-enterprise visibility into users and usage, integration processes, and the applications they connect



4
Workato
Workato's platform is extensible to add support for new applications beyond the many that are available pre-built. A powerful and configurable REST data connector allows integration with many systems without writing any code. 



5
Workato
Workato also provides a Connector SDK that handles and abstracts multiple aspects of application and data integration for cases where custom development is required. Existing connectors can be extended via the SDK or via adding functions from the REST connector



6
Workato
Recipes are automated workflows built by users that can span multiple apps, such as moving new accounts from Salesforce into Zendesk as new organizations. Each recipe comprises of a trigger and one or more actions that are carried out when a trigger event is picked up.



7
Workato
Triggers determine what event to listen to execute the actions described in a recipe.

Trigger events can be set off in apps (Salesforce or JIRA) when a certain event happens (new contact is created or existing ticket updated), when a new line is added in a file, or according to a schedule (fires at a certain time or interval).



8
Workato
Every step, including triggers, brings data into the recipe. For example a new employee in Workday trigger would bring in employee data. This data is made available in the recipe via the datatree.

The individual data fields are called datapills. You can use the datapills in subsequent steps.



9
Workato




10
Workato




11
Workato




12
Workato




13
Workato




14
Workato




15
Workato




16
Workato




17
Workato




18
Workato




19
Workato




20
Workato




21
Workato




22
Workato




23
Workato




24
Workato




25
Workato




26
Workato




27
Workato




28
Workato




29
Workato




30
Workato



