



11
Azure Data Fundamentals
Batch processing is suitable for handling large datasets efficiently. Stream processing is intended for individual records or micro batches consisting of few records.




12
Azure Data Fundamentals
Latency is the time taken for the data to be received and processed. The latency for batch processing is typically a few hours. Stream processing typically occurs immediately, with latency in the order of seconds or milliseconds.




13
Azure Data Fundamentals
You typically use batch processing to perform complex analytics. Stream processing is used for simple response functions, aggregates, or calculations such as rolling averages.




14
General
In computing, tar is a computer software utility for collecting many files into one archive file, often referred to as a tarball, for distribution or backup purposes. The name is derived from "tape archive", as it was originally developed to write data to sequential I/O devices with no file system of their own. 




15
General
When you download the source for libpri, DAHDI, and Asterisk you'll typically end up with files with a .tar.gz or .tgz file extension. These files are affectionately known as tarballs. The name comes from the tar Unix utility, which stands for tape archive. A tarball is a collection of other files combined into a single file for easy copying, and then often compressed with a utility such as GZip.


16
Azure Data Fundamentals
At its simplest, a high-level architecture for stream processing looks like this:
  1. An event generates some data
  2. The generated data is captured in a streaming source for processing
  3. The event is processed, often by a perpetual query.
  4. The results of the stream processsing operation are written to an output (or sink).



17
Azure Data Fundamentals
Microsoft Azure supports multiple technologies that you can use to implement real-time analytics of streaming data, including:

Azure Stream Analytics: A platform-as-a-service (PaaS) solution that you can use to define streaming jobs that ingest data from a streaming source, apply a perpetual query, and write the results to an output.


18
Azure Data Fundamentals
Microsoft Azure supports multiple technologies that you can use to implement real-time analytics of streaming data, including:

Azure Data Explorer: A high-performance database and analytics service that is optimized for ingesting and querying batch or streaming data with a time-series element, and which can be used as a standalone Azure service or as an Azure Synapse Data Explorer runtime in an Azure Synapse Analytics workspace.


19
Azure Data Fundamentals
The following services are commonly used to ingest data for stream processing on Azure:

  Azure Event Hubs: A data ingestion services that you can use to manage queues of event data, ensuring that each event is processed in order, exactly once.


20
Azure Data Fundamentals
The following services are commonly used to ingest data for stream processing on Azure:

  Apache Kafka: An open-source data ingestion solution that is commonly used together with Apache Spark. You can use Azure HDInsight to create a Kafka cluster.


21
Azure Data Fundamentals
The output from stream processing is often sent to the following services:

Azure Event Hubs
Azure Data Lake Store Gen 2 or Azure blob storage
Azure SQL Database or Azure Synapse Analytics, or Azure Databricks
Microsoft Power BI


22
Azure Data Fundamentals
Azure Stream Analytics is a great technology choice when you need to continually capture data from a streaming source, filter or aggregate it, and send the results to a data store or downstream process for analysis and reporting.



23
Azure Data Fundamentals
If your stream process requirements are complex or resource-intensive, you can create a Stream Analysis cluster, which uses the same underlying processing engine as a Stream Analytics job, but in a dedicated tenant (so your processing is not affected by other customers) and with configurable scalability that enables you to define the right balance of throughput and cost for your specific scenario.


24
REFramework Project with Tabular Data
Configuration steps:
  1. Ensure prerequisites are covered
  2. Create a new project and fill in the Config file
  3. CHange the TransactionItem datatype
  4. Configure the workflows used to open, close, and kill apps
  5. Edit the GetTransactionsData and Process workflows


25
REFramework Project with Tabular Data
When whiteboarding your process workflows, what information should you take into account?
  Workflow name
  Pre-condition
  Description
  Post-action
  Arguments
  State(s)


26
REFramework Project with Tabular Data
When working with tabular data, where would you set the MaxRetryNumber?
In the Constants sheet of the Config file

NOT In the Settings sheet of the Config file





27
REFramework Project with Tabular Data
By default, which states are affected by the TransactionItem datatype change?

  Get Transaction Data
  Process Transaction
  NOT Initialization
  NOT End Process


28
REFramework Project with Tabular Data
CloseAllApplications.xaml is used to log out and close the target applications

KillAllProcesses.xaml is used to force-close the target applications




29
REFramework Project with Tabular Data
If the TransactionData variable is of type DataTable, what type should the in_TransactionItem argument in Process.xaml have?
  DataRow
  NOT QueueItem
  NOT String
  NOT DataTable
  
30
REFramework Project with Tabular Data
We are planning to build an execution report functionality which includes the transaction identifier and the transaction status. What workflow would this logic best be included in?
  
  SetTransactionStatus.xaml 
  NOT GetTransactionData.xaml
  NOT RetryCurrentTransaction.xaml


1
REFramework Project with Tabular Data
[True]When working with REFramework template, you won't be able to create new test cases


2
REFramework Project with Tabular Data
You have just set the TransactionItem type to MailMessage. Which of the following types is a valid option for TransactionData?
List(of MailMessage)



3
REFramework Project with Tabular Data
You want to configure the REFramework template to build a linear process. Which argument do you need to monitor in GetTransactionData.xaml?
  in_TransactionNumber
  NOT in_Config
  NOT out_TransactionItem

4
REFramework Project with Tabular Data
What will be the type of TransactionItem if TransactionData is an Array of Strings?
  String
  NOT String[]



5
REFramework Project with Tabular Data
What action you need to take if you want to enable the MaxConsecutiveSystemExceptions feature in the REFramework template?
Set the MaxConsecutiveSystemExceptions constant value to greater than zero



6
REFramework Project with Tabular Data
Which argument in the Process workflow holds the business context information? 
In_TransactionItem


7
REFramework Project with Tabular Data
If we intend to use tabular data instead of an Orchestrator queue, which activity do we need to replace in the default REFramework template to retrieve the transaction data? 
The Get Transaction Item  activity in the Get Transaction Data state.





8
REFramework Project with Tabular Data
What is the default type of the 'TransactionData' variable?
DataTable
NOT Queue Item




9
REFramework Project with Tabular Data
The process is based on the REFramework template. Asset "Userdetails" was referenced in the config file, but it wasn't created in Orchestrator. What happens when you run the process?

  It throws an error in the Initialization state.




10
REFramework Project with Tabular Data
Is it possible to use tabular or other types of data for the transaction item and Orchestrator assets in the same process?

Yes. You can use Assets independently from Queues. 




11
REFramework Project with Tabular Data
From the given options, identify in which SetTransactionStatus workflow file is invoked.

  In the Try block of the process transaction state.
  Within the Business Rule Exception section in the catch section of process transaction.
  Within the System Exception section in the catch section of process transaction.






12
REFramework Project with Tabular Data
From the given option, identify the test cases available by default with REFramework template.
  GeneralTestCase
  GetTransactionDataTestCase
  MainTestCase
  ProcessTestCase
  InitAllApplicationsTestCase
  InitAllSettingsTestCase




13
Azure Data Fundamentals
Spark can be used to run code (usually written in Python, Scala, or Java) in parallel across multiple cluster nodes, enabling it to process very large volumes of data efficiently. Spark can be used for both batch processing and stream processing.



14
Azure Data Fundamentals
Spark Structured Stereaming is built on the ubiquitous structure in Spark called a dataframe, which encapsulates a table of data. You use the Spark Structured Streaming API to read data from a real-time data source, such as Kafka hub, a file store, or a network port, into a "boundless" dataframe that is continuallly populated iwth new data from the stream


15
Azure Data Fundamentals
Delta Lake is an open-source storage layer that adds support for transactional consistancy, schema enforcement, and other common data warehousing features to data lake storage.  It also unifies storage for streaming and batch data, and can be used in Spark to define relational tables for both batch and stream processing.


16
Azure Data Fundamentals
Azure Data Explorer is a standalone Azure service for efficiently analyzing data. You can use the service as the output for analyzing large volumes of diverse data from data sources such as websites, applications, IoT devices, and more. 


17
Azure Data Fundamentals
Data Explorer supports batcing and streaming in near real time to optimize data ingestion. The ingested data is stored in tables in a Data Explorer database, wehre automatic indexing enables high-performance queries.


18
Azure Data Fundamentals
To query Data Explorer tables, you can use Kusto Query Language (KQL), a language that is specifically optimized for fast read performance – particularly with telemetry data that includes a timestamp attribute.


19
Azure Data Fundamentals
You can add clauses to a Kusto query to filter, sort, aggregate, and return (project) specific columns. Each clause is prefixed by a | character. 

  LogEvents
  | where StartTime > datetime(2021-12-31) 
  | where EventType == 'Error'
  | project StartTime, EventType , Message


20
Azure Data Fundamentals
Which definition of stream processing is correct?

Data is processsed continually as new data records arrive


21
Azure Data Fundamentals
Which service would you use to continually capture data from an IoT Hub, aggregate it over temporal periods, and store the results in Azure SQL Database?

Azure Stream Analytics


22
Azure Data Fundamentals
Which language would you use to query real-tie log data in Azure Synapse Data Explorer?

Kusto Query Language (KQL)
NOT SQL
NOT Python


23
Azure Data Fundamentals
After you've created data models and reports, you can publish them to the Power BI service; a cloud service in which reports can be published and interacted with by business users. You can also do some basic data modeling and report editing directly in the service using a web browser, but the functionality for this is limited compared to the Power BI Desktop tool.


24
Azure Data Fundamentals
Analytical models enable you to structure data to support analysis. Models are based on related tables of data and define the numeric values that you want to analyze or report (known as measures) and the entities by which you want to aggregate them (known as dimensions). 


25
Azure Data Fundamentals
Dimension tables represent the entities by which you want to aggregate numeric measures – for example product or customer. Each entity is represented by a row with a unique key value. The remaining columns represent attributes of an entity – for example, products have names and categories, and customers have addresses and cities.


26
Azure Data Fundamentals
One final thing worth considering about analytical models is the creation of attribute hierarchies that enable you to quickly drill-up or drill-down to find aggregated values at different levels in a hierarchical dimension.



27
Azure Data Fundamentals
1. Which tool should you use to import data from multiple data sources and create a report?

Power BI Desktop


28
Azure Data Fundamentals
2. What should you define in your data model to enable drill-up/down analysis?

A hierarchy


29
Azure Data Fundamentals
3. Which kind of visualization should you use to analyze pass rates for multiple exams over time?

A line chart


30
General 
What Is Enterprise Resource Planning (ERP)?
At its core, ERP is an application that automates business processes and provides insights and internal controls, drawing on a central database that collects inputs from departments including accounting, manufacturing, supply chain management, sales, marketing and human resources (HR).




1
General
A memorandum of understanding (MOU) or partnership agreement is a document between two parties that allows each group to outline their expectations of the project and its deliverables.



2
UiPath Mail Activities
In the Mail Activities Package, what are the activities that contain built-in filtering?

  Get Outlook Mail Message
  Get Exchange Mail Messages
  Get IMAP Mail Messages
  NOT get POP Mail Messages



3
UiPath Mail Activities
While retrieving Emails from Gmail which activity is used.

  Get IMAP Mail Message



4
UiPath Mail Activities
Emails can be used as the input to or the output from a process



5
UiPath Mail Activities
Email activities are contained inside the UiPath.Mail.Activities pack.



6
UiPath Mail Activities
By default, the pack provides specific activities for Exchange, IBM Notes, IMAP, Outlook, POP3, and SMTP.



7
UiPath Mail Activities
The object type we use for retrieved emails is System.Net.Mail.MailMessage.



8
UiPath Mail Activities
When building a process which uses email, for most activity groups, some configuration may be required on the email account.



9
UiPath Mail Activities
The Get IMAP Mail Messages activity lets us filter messages based on different criteria.



10
UiPath Mail Activities
A great resource to learn more about filtering emails is the Microsoft documentation page.



11
UiPath Mail Activities
To create an email template, write the message in a new text document and then use a 'Read Text' activity to read it.

To store the template text, we must create a new variable in the 'output to' property field


12
UiPath Mail Activities




13
UiPath Mail Activities




14
UiPath Mail Activities




15
UiPath Mail Activities




16
UiPath Mail Activities




17
UiPath Mail Activities




18
UiPath Mail Activities




19
UiPath Mail Activities




20
UiPath Mail Activities




21
UiPath Mail Activities




22
UiPath Mail Activities




23
UiPath Mail Activities




24
UiPath Mail Activities




25
UiPath Mail Activities




26
UiPath Mail Activities




27
UiPath Mail Activities




28
UiPath Mail Activities




29
UiPath Mail Activities




30
UiPath Mail Activities



