


20
Azure Data Fundamentals
The Table API is used to work with data in key-value tables, similar to Azure Table Storage. The Azure Cosmos DB Table API offers greater scalability and performance than Azure Table Storage.





21
Azure Data Fundamentals
The Cassandra API is compatible with Apache Cassandra, which is a popular open source database that uses a column-family storage structure. Column families are tables, similar to those in a relational database, with the exception that it's not mandatory for every row to have the same columns.





22
Azure Data Fundamentals
The Gremlin API is used with data in a graph structure; in which entities are defined as vertices that form nodes in connected graph. Nodes are connected by edges that represent relationships, like this:

https://docs.microsoft.com/en-us/learn/modules/explore-non-relational-data-stores-azure/3-cosmos-db-apis

The example in the image shows two kinds of vertex (employee and department) and edges that connect them (employee "Ben" reports to employee "Sue", and both employees work in the "Hardware" department).



23
Azure Data Fundamentals
Which API should you use to store and query JSON documents in Azure Cosmos DB?

Core (SQL) API is designed to store and query JSON documents.



24
Azure Data Fundamentals
Which Azure Cosmos DB API should you use to work with data in which entities and their relationships to one another are represented in a graph using vertices and edges?

The Gremlin API is used to manage a network of nodes (vertices) and the relationships between them (edges).



25
Azure Data Fundamentals
How can you enable globally distributed users to work with their own local replica of a Cosmos DB database?

Enable multi-region writes and add the regions where you have users. You can enable multi-region writes in the regions where you want users to work with the data.



26
Azure Data Fundamentals
A conventional data warehousing solution typically involves copying data from transactional data stores into a relational database with a schema that's optimized for querying and building multidimensional models.



27
Azure Data Fundamentals
Big Data processing solutions, are used with large volumes of data in multiple formats, which is batch loaded or captured in real-time streams and stored in a data lake from which distributed processing engines like Apache Spark are used to process it.



28
Azure Data Fundamentals
Analytical data model - while data analysts and data scientists can work with the data directly in the analytical data store, it's common to create one or more data models that pre-aggregate the data to make it easier to produce reports, dashboards, and interactive visualizations. Often these data models are described as cubes, in which numeric data values are aggregated across one or more dimesniosn (for example, to determine total sales by product an dregion).



29
Azure Data Fundamentals
Data Warehousing Architecture
  1. Data ingestion and processing
  2. Analytical data store
  3. Analytical data model
  4. Data visualization



30
Azure Data Fundamentals
On Azure, large-scale data ingestion is best implemented by creating pipelines that orchestrate ETL processes. You can create and run pipelines using Azure Data Factory, or you can use the same pipeline engine in Azure Synapse Analytics




Process finished with exit code 0
1
Azure Data Fundamentals
Pipelines consist of one or more activities that operate on data. An input dataset provides the source data, and activities can be defined as a data flow that incrementally manipulates the data until an output dataset is produced. Pipelines use linked services to load and process data â€“ enabling you to use the right technology for each step of the workflow


2
Azure Data Fundamentals
A fact and dimension table schema is called a star schema; though it's often extended into a snowflake schema by adding additional tables related to the dimension tables to represent dimensional hierarchies (for example, product might be related to product categories)


3
Azure Data Fundamentals
Technologies like Spark or Hadoop are often used to process queries on the stored files and return data for reporting and analytics. These systems often apply a schema-on-read approach to define tabular schemas on semi-structured data files at the point where the data is read for analysis, without applying constraints when it's stored


4
Azure Data Fundamentals
Databricks provides an interactive user interface through which the system can be managed and data can be explored in interactive notebooks.




5
Azure Data Fundamentals
Data processing is simply the conversion of raw data to meaningful information through a process. there are two general ways to process data:

  Batch processing, in which data records are collected and stored before being processed together in a single operation.


6
Azure Data Fundamentals
Data processing is simply the conversion of raw data to meaningful information through a process. there are two general ways to process data:

  Stream processing, in which a source of data is constantly monitored and processed in real time as new data events occur.


7
Azure Data Fundamentals
Advantages of batch processing include:

  Large volumes of data  can be processed at a convenient time.
  It can be scheduled to run at a time when computers or systems might otherwise be idle, such as overnight or during off-peak hours.


8
Azure Data Fundamentals
Disadvantages of batch processing include:

  The time delay between ingesting data and getting the results.

  All of a batch job's input data must be ready before a batch can be processed. This means data must be carefully checked. Problems with data, errors, and program crashes that occur during batch jobs bring the whole process to a halt.


9
Azure Data Fundamentals
In stream processing, each new piece of data is processed when it arrives. Unlike batch processing, there's no waiting until the next batch processing interval - data is processed as individual units in real-time rather than being processed a batch at a time.


10
Azure Data Fundamentals
Batch processing can process all the data in the dataset. Stream processing typically only has access to the most recent data received, or within a rolling time window (the last 30 seconds, for example).




11
Azure Data Fundamentals
Batch processing is suitable for handling large datasets efficiently. Stream processing is intended for individual records or micro batches consisting of few records.




12
Azure Data Fundamentals
Latency is the time taken for the data to be received and processed. The latency for batch processing is typically a few hours. Stream processing typically occurs immediately, with latency in the order of seconds or milliseconds.




13
Azure Data Fundamentals
You typically use batch processing to perform complex analytics. Stream processing is used for simple response functions, aggregates, or calculations such as rolling averages.




14
General
In computing, tar is a computer software utility for collecting many files into one archive file, often referred to as a tarball, for distribution or backup purposes. The name is derived from "tape archive", as it was originally developed to write data to sequential I/O devices with no file system of their own. 




15
General
When you download the source for libpri, DAHDI, and Asterisk you'll typically end up with files with a .tar.gz or .tgz file extension. These files are affectionately known as tarballs. The name comes from the tar Unix utility, which stands for tape archive. A tarball is a collection of other files combined into a single file for easy copying, and then often compressed with a utility such as GZip.


16
Azure Data Fundamentals
At its simplest, a high-level architecture for stream processing looks like this:
  1. An event generates some data
  2. The generated data is captured in a streaming source for processing
  3. The event is processed, often by a perpetual query.
  4. The results of the stream processsing operation are written to an output (or sink).



17
Azure Data Fundamentals
Microsoft Azure supports multiple technologies that you can use to implement real-time analytics of streaming data, including:

Azure Stream Analytics: A platform-as-a-service (PaaS) solution that you can use to define streaming jobs that ingest data from a streaming source, apply a perpetual query, and write the results to an output.


18
Azure Data Fundamentals
Microsoft Azure supports multiple technologies that you can use to implement real-time analytics of streaming data, including:

Azure Data Explorer: A high-performance database and analytics service that is optimized for ingesting and querying batch or streaming data with a time-series element, and which can be used as a standalone Azure service or as an Azure Synapse Data Explorer runtime in an Azure Synapse Analytics workspace.


19
Azure Data Fundamentals
The following services are commonly used to ingest data for stream processing on Azure:

  Azure Event Hubs: A data ingestion services that you can use to manage queues of event data, ensuring that each event is processed in order, exactly once.


20
Azure Data Fundamentals
The following services are commonly used to ingest data for stream processing on Azure:

  Apache Kafka: An open-source data ingestion solution that is commonly used together with Apache Spark. You can use Azure HDInsight to create a Kafka cluster.


21
Azure Data Fundamentals
The output from stream processing is often sent to the following services:

Azure Event Hubs
Azure Data Lake Store Gen 2 or Azure blob storage
Azure SQL Database or Azure Synapse Analytics, or Azure Databricks
Microsoft Power BI


22
Azure Data Fundamentals
Azure Stream Analytics is a great technology choice when you need to continually capture data from a streaming source, filter or aggregate it, and send the results to a data store or downstream process for analysis and reporting.



23
Azure Data Fundamentals
If your stream process requirements are complex or resource-intensive, you can create a Stream Analysis cluster, which uses the same underlying processing engine as a Stream Analytics job, but in a dedicated tenant (so your processing is not affected by other customers) and with configurable scalability that enables you to define the right balance of throughput and cost for your specific scenario.


24
REFramework Project with Tabular Data



25
REFramework Project with Tabular Data



26
REFramework Project with Tabular Data



27
REFramework Project with Tabular Data



28
REFramework Project with Tabular Data



29
REFramework Project with Tabular Data



30
REFramework Project with Tabular Data