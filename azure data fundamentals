sIn addition to tables, a relational database can contain other structures that help to optimize data organization, encapsulate programmatic actions, and improve the speed of access. IN this unit, you'll learn about three of these structures in more detail: views, stored procedures, and indexes.

What is a view?

A view is a virtual table based on the results of a SELECT query. You an think of a view as a window on specified rows in one or more underlying tables. For example, you could create a view on the Order and Customer tabels that retrieves order and customer data to provide a single object that makes it easy to determine delivery addresses for orders.

CREATE VIEW Deliveries
AS
SELECT o.OrderNo, o.OrderDate,
  c.FirstName, c.LastName, c.Address, c.City
from Order AS o JOIN Customer AS c
ON o.Customer = c.ID;
You can query the view and filter the data in much the same way as a table. The following query finds details in orders for customers who live in Seattle:

SELECT OrderNo, OrderDate, LastName, Address
FROM Deliveries
WHERE City = 'Seattle';

What is a stored procedure?

A stored procedure defines SQL statements that an be run on command. Stored procedures are used to encapsulate programmatic logic in a database for actions that applications need to perform when working with data.

You can define a stored procedure with parameters to create a flexible solution for common actions that might need to be applied to data based on a specific key or criteria. For example, the following stored procedure could be defined to change the name of a product based on the specified product ID.

CRREATE PROCEDURE RenameProduct
  @ProductID INT,
  @NewName VARCHAR(20)
AS
UPDATE Product
SET Name = @NewName
WHERE ID = @ProductID

When a product must be renamed, you can execute the proedure, passing the ID of the product and the new name to be assinged.

EXEC RenameProduct 201, 'Spanner';

What is an index?

An index helps you search for data in a table. Think of an index over a table like an index at the back of a book. A book index contains a sorted set of references, with the pages on which each reference occurs. When you want to find a reference to an item in a book, you look it up through the index. You can use the page numbers in the index to directly to the correct pages in the book. Without an index, you might have to read through the entire book to find the references you're looking for.

WHen you create an index in a database, you specify a column from the table, and the index contains a copy of this data in a sorted order, with pointers to the corresponding rows in the table. When the user runs a query that specifies this column in the WHERE clause, the database management system can use this index to fetch the data more quickly than if it had to scan the entire table row by row.

For example, you could use the following code to create an index on the Name column of the Product table.

CREATE INDEX idx_ProductName
ON Product(NAME);

The index creates a tree-based structure that the database system's query optimizer can use to quickly find rows in the Product table based on a specified Name.

For a table containing few rows, using hte index is probably not any more efficient than simply reading the entire table and finding the rows requested by the query (in which case the query optimizer will ignore the index). However, when a table has many rows, the indexes can dramatically imporve the performance of queries.

You can create many indexes on a table. So, if you also wanted to find products based on price, creating another Index on the Price column in the Product table might be useful. However, Indexes aren't free. An index consumes storage space, and each time you insert, update, and delte data in a table, teh indexes for that table must be maintained. The additional work can slow down insert, update, and delete operations. You must strike a balance between having indexes that speed up your queries versus the cost of performing other operations.

https://docs.microsoft.com/en-us/learn/modules/explore-provision-deploy-relational-database-offerings-azure/2-azure-sql

...


SQL Server on Azure Virtual Machines

SQL Server on Virtual Machines enables you to use full versions of SQL Server in the Cloud without having to manage any on premises hardware. This is an example of the IaaS approach.

SQL Server running on an Azure virtual machine effectively replicates the database running on real on-premises hardware. Migrating from the system running on-premises to an Azure virtual machine is no different than mocing the databases from one on-premises server to another.

This approach is suitable for migrations and applications requiring access to operating system features that might be unsupported at the PaaS level. SQL virtual machines are lift-and-shift ready for existing applications that require fast migration to the cloud with minimal changes.  YOu can also use SQL Server on Azure VMs to extend existing on-premises applications to the cloud in hybrid deployments.

NOTE
A hybrid deployment is a system where part of the operation runs on-premises and part in the cloud. Your database might be part of a larger system that runs on-premises, although the database elements might be hosted in the cloud.

You can use SQL Server in a virtual machine to develop and test traditional SQL Server applications. With a virtual machine, you have the full administrative rights over the DBMS and operating system. It's a perfect choice when an organization already has IT resources available to maintain the virtual machines.

These capabilities enable you to:

...

Business benefits

Azure SQL Database automatically updates and patches the SQL Server software to ensure that you're always running the latest andmost secure version of the service. The scalability features of Azure SQL Database ensure that you can increase the resources available to store and process data without having to perform a costly manual upgrade.

The service provides high avaialability guarantees, to ensure that your databases are avialable at least 99.995% of the time. Azure SQL Database supports point-in-time restore, enabling you to recover a database to the state it was in at any point in the past. Databases can be replicated to different regions to provide more resiliency and disaster recovery.

Advanced threat protection provides advanced security capabilities, such as vulnerability assessments, to help detect and remediate potential security problems with your databases. Threat protection also detects anomalous activities that indicate unusual and potentially harmful attempts to access or exploit your database. It continuously monitors your database for suspicious activity, and recommended action on hwo to investigate and mitigate the threat.

Auditing tracks database events and writes them to an audit log in your Azure storage account. Auditing can help you maintainregulatory complaince, understand database activity, and gain insight into discrepancies and anomalies that might indicate business concerns or suspected security violations.

SQL Database helps secure your data by providing encryption that protects data that is stored in the database (at rest) and while it is being transferred across the network (in motion).

https://docs.microsoft.com/en-us/learn/modules/explore-provision-deploy-relational-database-offerings-azure/3-azure-database-open-source

Describe Azure services for open-source databases

In addition to Azure SQL services, Azure data services are available for other popular relational database systems, including MySQL, MariaDB, and PostgreSQL. The primary reason for these services is to enable organizations that use them in on-premises apps to move to Azure quickly, without making significan changes to their applications.

What are MySQL, MariaDB, and PostgreSQL?

My SQL, MariaDB, and PostgreSQL are relational database management systems that are tailored for different specializations.

MySQL started life as a simple-to-use open-source database management system. It's the leading open source relational database for Linux, Apache, MySQL, and PHP (LAMP) stack apps. It's available in several editions; Community, Standard, and Enterprise. The Community edition is available free-of-charge, and has historically been popular as a database management sytem for web applications, running under Linux. Versions are also available for Windows. Standard edition offers higher performance, and uses a different technology for storing data. Enterprise edition provides a comprehensive set of tools and features, including enhanced securiity, availability, and scalability. The Standard and Enterprise editions are the versions most frequently used by commercial organizations, although these versions of the software aren't free.

MariaDB is a newer database management system, created by the original developers of MySQL. The database engine has since been rewritten and optimized to improve performance. MariaDB offers compatibility with Oracle Database (another popular commercial database management system). One notable feature of MariaDB is built-in support for temporal data. A table can hold several versions of data, enabling an application to query the data as it appeared at some point in time.

PostgreSQL is a hybrid relational-object database. You can store data in relational tables, but a PostgreSQL database also enables you to store custom data types, with their own non-relational properties. The database management system is extensible; you can add code modules to the database, which can be run in queries. Another key feature is the ability to store and manipulate geometric data, such as lines, cercles, and polygons.

PostgreSQL has its own query language called pgsql. This language is a variant of the standard relational query language, SQL, with features that allow you to write stored procedures taht run inside the database.

Azure Database for MySQL

...

Azure Database for MySQL is a Paas implementation of MySQL in the Azure cloud, based on MySQL Community Edition.

The Azure Database for MySQL service includes highly availability at no additional cost, and scalability as required. You only pay for what you use. Automatic backups are provided, with point-in-time restore.

The server provides connection security to enforce firewall rules, and, optionally, require SSL connections. Many server parameters enable you to configure server settings such as lock modes, maximum number of connections, and timeouts.

Azure Database for MySQL provides a global database system that scales up to large databases without the need to manage hardware, network components, virtual servers, software patches, and other underlying components.

Certain operations aren't available with Azure Database for MySQL. These functions are primarily concerned with security and administration. Azure manages these aspects of the database server itself.

...

Azure Database for PostgreSQL

If you prefer PostgreSQL, you can choose Azure Databatse for PostgreSQL to run a PaaS implementation of PostgreSQL in the Azure Cloud. THis service provides thes ame availability, performance, scaling, security, and administrative benefits as the MySQL Server.

Some features of on-premises PostgreSQL databases aren't available in Azure Database for PostgreSQL. These features are monstly concerned with the extensions that users can add to a database to perform specialized tasks such as writing stored procedures in various programming languages (other than pgsql, which is available), and interacting directly with the operating system. A core set of the most frequently used extensions is supported, and the list of available extensions is under continuous review.

Azure Database for PostgreSQL has three deployment options: Single Server, Flexible Server, and Hyperscale.

Azure Database for PostgreSQL Single Server

The single-server deployment option for PostgreSQL provides similar benefits as Azure Database for MySQL. You choose from three pricing tiers: Basic, General Purpose, and Memory Optimized. Each tier supports different numbers of CPUs, memory, and storage sizes; you select one base do on the load you expect to support.

Azure Database for PostgreSQL Flexible Server

The flexible-server deployment option fo PostgreSQL is a fully managed database service. It provides more control and server configuration customizations, and has better cost optimization controls.

Azure Database for PostgreSQL Hyperscale (Citus)

Hyperscale (Citus) is a deployment option that scales queries across multiple server nodes to support large database loads. Your database is split across nodes. Data is split into hunks based on the value of a partition key or sharding key. Consider using this deployemnt option for the largest database PostgreSQL deployments in the Azure Cloud.

Doing the azure lab; recording information

Name of server
samplesql-afjh

Server admin login
endeavorlycanroc

Server Password
Def Ferrothorn: 486

https://docs.microsoft.com/en-us/learn/modules/explore-provision-deploy-non-relational-data-services-azure/6-exercise-azure-storage

Exercise: Explore Azure Storage

Now it's your opportunity to explore Azure STorage.

This exercise can be completed using a Microsoft learn...

Provision an Azure Storage account

The first step in using Azure Storage is to provision an Azure Storage account in your Azure subscription.

1. If you haven't already done so, sign into the Azure portal at https://portal.azure.com/. Then on the Azure portal home page, select + Create a resource from the upper left-hand corner and search for Storage account. THen in the resulting Storage account page, select Create.

2. Enter the following values on the Create a storage account page:

  Subscription: If you're using a sandbox, select Concierge Subscription. Otherwise, select your Azure subscription.

  Resource group: If you're using a sandbox, select the existing resource group (which will have a name like learn-xxx...). Otherwise, create a new resource group with a name of your choice.

  Storage account name: Enter a unique name for your storage accoung using lower-case letters and numbers.

  Region: Select any available location.

  Performance: Standard

  Redundancy: Locally-redundant storage (LRS)

3. Select Next: Advanced > and view the advanced configuration options. In particular, note that this is where you can enable hierarchical namespace to support Azure Data Lack Storage Gen2. Leave this option unselected (you'll enable it later), and then select Next:Networking > to view the networking options for your storage account.

4. Select Next: Data protection > and then in the Recovery section, deselect all of the Enable soft delete... options. These options retain deleted files for subsequent recovery, but can cause issues later when you enable hierarchical namespace.

5. Continue through the remaining Next > pages without changing any of the default settings, and then on the Review + Create page, wait for your selections to be validated and select Create to create your Storage account.

6. Wait for deployment to complete. Then go to ther resource that was deployed.

Explore blob storage

Now that you have an Azure Storage account, you can create a container for blob data.

1. Download the product1.json JSON file from https://aka.ms/product1.json and save it to your computer (you can save it in any folder - you'll upload it to blob storage later).

...

3. In the DataLack Gen2 upgrade page, expand and complete each step to upgrade your storage account to enable hierarchical namespace and support Azure Data Lack Storage Gen 2. This may take some time.

4. When the upgrade is complete, in the pane on the left side, in the top section, select Storage browser (preview) and navigate back to the root of your data blob container, which still contains the product_data folder.

5. Select the product_data folder, and varify it still contains the product1.json file you uploaded previously.

6. Use the Upload Button to open the Upload blob panel.

7. IN the Upload blob panel, select the product2.json file you saved on your local computer. Then select the Upload button.

https://docs.microsoft.com/en-us/learn/modules/explore-non-relational-data-stores-azure/2-describe-azure-cosmos-db

Describe Azure Cosmos DB

Azure Cosmos DB supports multiple application programming interfaces (APIs) that enable developers to use programming semantics of many kinds of data store to work with data in a CosmosDB database. The internal data structure is abstracted, enabling developers to use CosmosDB to store and query data using APIs with which they're already familiar.

Note
An API is an Application Programming Interface. Database management systems (and other software frameworks) provide a set of APIs that developers can use to write programs that need to access data. The APIs vary for different database management systems.

Cosmos DB uses indexes and partitioning to provide fast read and write performance and can scale to massive volumes of data. You can enable multi-region writes, adding the Azure regions of your choice to your Cosmos DB account so that globally distributed users can each work with data in their local replica.

When to use Cosmos DB

Cosmos DB is a highly scalable database management system. Cosmos DB automatically allocates space in a container for your partitions, and each partition can grow up to 10 GB in size. Indexes are created and maintained automatically. There's virtually no administrative overhead.

Cosmos DB is a foundational service in Azure. Cosmos DB has been used by many of Microsoft's products for mission critical applications at global scale, including Skype, Xbox, Microsoft 365, Azure, and many others. Cosmos DB is highly suitable for the following scenarios:

  IoT and telematics. These systems typically ingest large amounts of data in frequent bursts of activity. Cosmos DB can accept and store this information quickly. The data can be used by analytics services, such as Azure Machine Learning, Azure HDInsight, and Power BI. Additionally, you can process data in real-time using Azure Functions that are triggered as data arrives in the database.

  Retail and marketing. Microsoft uses Cosmos DB for its own e-commerce platforms that run as part of Windows Store and Xbox Live. It's also used in the retail industry for storing catalog data and for event sourcing in order processing pipelines.

  Gaming. The database tier is a crucial component of gaming applications. Modern games perform graphical processing on mobile/console clients, but they rely on the cloud to deliver customized and personalized content like in-game stats, social media integration, and high-score leaderboards. Games often require single-millisecond latencies for reads and write to provide an engaging in-game experience. A game database needs to be fast and be able to handle massive spikes in request rates during new game launches and feature updates.

  Web and mobile applications. Azure Cosmos DB is commonly used within web and mobile applications, and it is well suited for modeling social interactions, integrating with third-party services, and for building rich personalized experiences. The Cosmos DB SDKs can be used to build rich iOS and Android applications using the popular Xamarin framework.

For additional information about uses for Cosmos DB, read Common Azure Cosmos DB use cases.

https://docs.microsoft.com/en-us/learn/modules/explore-non-relational-data-stores-azure/3-cosmos-db-apis

Identify Azure Cosmos DB APIs

Azure Cosmos DB supports multiple APIs, enabling developers to easily migrate data from commonly used NoSQL stores and apply their existing programming skills. When you provision a new Cosmos DB instance, you select the API that you want to use. The choice of API depends on many factors including the type of data to be stored, the need to support existing applications, and the API skills of the developers who will work with the data store.

Core (SQL) API

The native API in Cosmos DB manages data in JSON document format, and despite being a NoSQL data storage solution, uses SQL syntax to work with the data.

A SQL query for a Cosmos DB database containing customer data might look similar to this:

  SELECT *
  FROM customers c
  WHERE c.id = "joe@litware.com"

Theh result of this query consists of one or more JSON documents, as shown here:

  {
    "id": "joe@litware.com",
    "name": "Joe Jones",
    "address": {
        "street": "1 Main St",
        "city": "Seattle"
    }
  }

MongoDB API

MongoDB is a popular open source database in which data is stored in Binary JSON (BSON) format. The Azure Cosmos DB MongoDB API enables developers to use MongoDB client libraries and code to work with data in Azure Cosmos DB.

MongoDB Query Language (MQL) uses a compact, object-oriented syntax in which developers use objects to call methods. For example, the following query uses the find method to query the producs collection in the db object:

  db.products.find({id: 123})

The results of this query consist of JSON documents, similar to this:

  {
      "id": 123,
      "name": "Hammer",
      "price": 2.99
  }

Table API

The Table API is used to work with data in key-value tables, similar to Azure Table Storage. The Azure Cosmos DB Table API offers greater scalability and performance than Azure Table Storage.

For example, you might define a table named customers like this:

  PartitionKey	RowKey	Name	Email
  1	123	Joe Jones	joe@litware.com
  1	124	Samir Nadoy	samir@northwind.com

You can then use the Cosmos DB Table API through one fo the language-specific SDKs to make calls to your service endpoint to retrieve data from the table. For example, the following request returns the row containing the record for Samir Nadoy in teh tabel above:

  https://endpoint/Customers(PartitionKey='1', RowKey='124')

https://docs.microsoft.com/en-us/learn/modules/explore-non-relational-data-stores-azure/4-exercise-explore-cosmos-db

Create a Cosmos DB account

To use Cosmos DB, you must provision an Cosmos DB account in your Azure subscription. In this exercise, you'll provision a Cosmos DB account that uses the core (SQL) API.

  1. IN the Azure portal, select + Create a resource at the top left, and search for Azure Cosmos DB. In the results, select Azure Cosmos DB and select Creat.
  2. In the Core (SQL) - Recommended tile, select Create.
  3. Enter the following details, and then select Review + Create:

    Subscription: If you're using a sandbox, select Concierge Subscription. Otherwise, select your Azure subscription.
    Resource group: If you're using a sandbox, select the existing resource group (which will have a name like learn-xxxx...). Otherwise, create a new resource group with a name of your choice.
    Account Name: Enter a unique name.
    Location: Choose any recommended location
    Capacity mode: Provisioned throughput
    Apply Free-Tier Discount: Select Apply if available
    Limit total account throughput: Unselected

  4. When the configuration has been validated, select Create.
  5. Wait for deployment to complete. Then go to the deployed resource.

Create a simple database

Throughout this procedure, close any tips that are displayed in the portal.

  1. On the page for your new Cosmos DB account, in the pane on the left, select Data Explorer.
  2. In the Data Explorer page, select Launch quick start.
  3. In the New Container tab, review the

https://docs.microsoft.com/en-us/learn/modules/examine-components-of-modern-data-warehouse/2-describe-warehousing

...

2. Analytical data store - data stores for large scale analytics include relational data warehouses, file-system based data lakes, and hybrid architectures that combine features of data warehouses and data lakes (sometimes called data lakehouses or lake databases). We'll discuss these in more depth later.

3. Analytical data model - while data analysts and data scientists can work with the data directly in the analytical data store, it's common to create one or more data models that pre-aggregate the data to make it easier to produce reports, dashboards, and interactive visualizations. Often these data models are described as cubes, in which numeric data values are aggregated across one or more dimesniosn (for example, to determine total sales by product an dregion). The model encapsulates the relationships between data values and dimensional entities to support "drill-up/drill-down" analysis.

4. Data visualization - Data analysts consume data from analytical models, and directly from analytical stores to create reports, dashboards, and other visualizations. Additionally, users in an organization who may not be technology professionals might perform self-service data analysis and reporting. The visualizations from the data show trends, comparisons, and key performance indicators (KPIs) for a business or other organization, and can take the form of printed reports, graphs and charts in documents or PowerPoint presentations, web-based dashboards, and interactive environments in which users can explore data visually.

https://docs.microsoft.com/en-us/learn/modules/examine-components-of-modern-data-warehouse/3-data-ingestion-pipelines

Explore data ingestion pipelines

Now that you understand a little about the architecture of a large-scale data warehousing solution, and some of the distributed processing technologies that can be used to handle large volumes of data, it's time to explore how data is ingested into an analytical data store from one or more sources.

On Azure, large-scale data ingestion is best implemented by creating pipelines that orchestrate ETL processes. You can create and run pipelines using Azure Data Factory, or you can use the same pipeline engine in Azure Synapse Analytics if you want to manage all of the components of your data warehousing solution in a unified workspace.

In either case, pipelines consist of one or mroe activities that operate on data. An input dataset provides the source data, and activities can be defined as a data flow that incrementally manipulates the data until an output dataset is produced. Pipelines use linked services to load and process data - enabling you to use the right technology for each step of the workflow. For example, you might use an Azure Blob Store linked service to ingest the input dataset, an dthen use services such as Azure SQL Database to run a stored procedure that looks up related data valules, before running a data procesing task on Azure Databricks or Azure HDInsight, or apply custom logic using an Azure Function. Finally, you can save the output dataset in a linked service such as Azure Synapse Analytics. Pipelines can also includ some built-in activities, which don't require a linked service.

https://docs.microsoft.com/en-us/learn/modules/examine-components-of-modern-data-warehouse/4-analytical-data-stores

Explore analytical data stores

A data warehouse is a relational database in which the data is stored in a schema that is optimized for data analytics rather than transactional workloads. Commonly, the data from a transactional store is transformed into a schema in which numeric values are stored in central fact tables, which are related to one or more dimension tables that represent entities by which the data can be aggregated. For example, a fact table might contain sales order data, which can be aggregated by customer, product, store, and time dimensions (enabling you, for example, to easily find the montly total sales revenue by product for each store). THis kind of fact and dimension tables chema is called a star schema; though it's often expanded into a snowflake schema by adding additional tables related to the dminsion tables to represent dimensional hierarchies (for example, product might be related to product categories). A data warehouse is a great chioce when you have transactionald ata that can be organized into a structred shema of tables, and you want to use SQL to query them.

Data lakes

A data lake is a file store, usually on a distributed file system for high performance data access. Technologies like Spark or Hadoop are often used to proces queires on the stored files and return data for reporting and analytics. These systems apply a schema-on-read approach to define tabular schemas on semi-structured data files at the point where the data is read for analysis, without applying constraints when it's stored. Data lakes are great for supporting a mix of structured, semi-structured, and even unstructured data that you want to analyze without the need for schema enforemcent when the data is written to the store.

Hybrid approaches.

You can use a hybrid approach that combines features of data lakes and data warehouses in a lake database or data lakehouse. The raw data is stored as fiels in a data lake, and a relational storage layer abstracts the underlying files and expose them as tables, which can be queried using SQL. SQL pools in Azure Synapse Analytics include PolyBase, which enables you to define external tables based on files in a datalake (and other sources) and query them using SQL. Synapse Analytics also supports a Lake Database approach in which you can use database templates to define the relational schema of ryour data warehouse, while storing the underlying data in data lake storage - separating the storage and compute for your data warehousing solution.

================

Azure services for analytical stores

On Azure, there are three main services you can use to implement a large-scale analytical store.

Azure Synapse Analytics is a unified, end-to-end solution for large scale data analytics. It brings together multiple technologies and capabilities, enabling you to combine the data integrity and flexibility of a scalable, high-performance SQL Server based relational data warehouse with the flexibility of a data lake and open-source Apache Spark. It also includes native support for log and telemetry analytics with Azure Synapse Data Explorer pools, as weel as built in data pipelines for data ingestion and transformation. All Azure Synapse Analytics services can be managed through a single, interactive user interface called Azure Synapse Studio, which includes the ability to create interactive notebooks in which Spark code and markdown content can be combined. Synapse Analytics is a great choice when you want to create a single, unified analytics solution on Azure.

Azure Databricks is an Azure implementation of the popular Databricks platform. Databricks is a comprehensive data analytics solution built on Apache Spark, and offers native SQL capabilities as well as workload-optimized Spark clusters for data analytics and data science. Databricks provides an interactive user interface through which the system can be managed and data can be explored in interactive notebooks. Due to its common use on multiple cloud platforms, you might want to consider using Azure Databricks as your analytical store if you want to use existing expertise with the platform or if you need to operate in a multi-cloud environment or support a cloud-portable solution.

Azure HDInsight is an Azure service that supports multiple open-source data analytics cluster types. Although not as user-friendly as Azure Synapse Analytics and Azure Databricks, it can be a suitable option if your analytics solution relies on multiple open-source frameworks or if you need to migrate an existing on-premises Hadoop-based solution to the cloud.

NOTE

Each of these services can be thought of as an analytical data store, in the sense that they provide a schema and interface through which teh data can be queried. IN many cases, however, the data is actually stored in a data lake and the service is used to process the data and run the queries. SOme solutions might even combine the use of these services. An extract, load, and transform (ELT) ingestion process might copy data into the data lake, and then use one of these services to transform data, and another to query it. For example, a pipeline might use a MapReduce job running in HDInsight or a notebook running in Azure Databricks to process a large volume of data in the data lake, and then load it into tables in the SQL pool in Azure Synapse Analytics.

https://docs.microsoft.com/en-us/learn/modules/examine-components-of-modern-data-warehouse/5-exercise-azure-synapse

Exercise Explore Azure Synapse analytics

In this exercise, you'll create an Azure Synapse Analytics workspace and use it to ingest and analyze some data.

The exercise is designed to familiarize you with some key elements of a large-scale data warehousing solution, not as a comprehensive guide to performing advanced data analytics with Azure Synapse Analytics. The exercise should take around 30 minutes to complete.

Note
To complete this article, you'll need a Microsoft Azure subscription. If you don't already have one, you can sigh up for a free trial at https://azure.microsoft/free. You cannot use a Microsoft Learn sandbox subscription for this exercise.

Provision an Azure Synapse Analytics workspace

To use Azure Synapse Analytics, you must provision and Azure Synapse Analytics Workspace resource in your Azure subscription.

1. Open the Azure portal at https://portal.azure.com/ and sign in using the credentials associated with your Azure subscription.

Tip
Ensure you are working in the directory containing your subscription - indicated at the top right under your user ID. If not, select the user icon and switch directory. Note that if you previously used a Microsoft Learn sandbox subscription, the portal may have defaulted to Microsoft Learn Sandbox directory. If so, you'll need to switch to your own directory.

...

Note

A Synapse Analytics worwkspace requires two resource groups in your Azure subscription; one for the resources you explicitly create, and another for managed resources used by the service. It also requries a Data Lake storage account in which to store data, scripts, and other artifacts.

4. When you've entreed these details, select Review + create, and then select Create to create the workspace.

5. Wait for the workspace to be created - this may take five minutes or so.

6. When deployement is complete, go to the resource group that was created and notice that it contains your Synapse Analytics workspace and Data Lake storage account.

7. Select your Synapse workspeace, and in its Overview page, in Open Synapse Studio card, select Open to use Synapse Studio in a new browser tab. Synapse Studio is a web-based interface that you can use to work with your Synapse Analytics workspace.

8. On the left side of Synapse Studio, use the >> icon to expand the menu - this reveas the different pages within Synapse Studio that you'll use to manage resources and perform data analytics tasks, as shown here:

Ingest data

One of the key tasks you can perform with Azure Synapse Analytics is to define pipelines that transfer (and if necessary, transform) data from a wide range of sources into your workspace for analysis.

1. In Synapse Studio, on the Home page, select Ingest and then choose Built-in copy task and open the Copy Data tool tool.

2. In the Copy Data tool, on the Properties step, ensure that Built-in copy task and Run once now are selected, and click Next >.

...

5. On the Source step, in the Configuration substep, select Preview data to see a preview of the product data your pipelin will ingest, then close the preview.

6. After previewing the data, on the Source/Configuration step, ensure the following settings are selected, and then select Next >:

  File format: Delimited Text
  Column delimiter: Comma (,)
  Row delimiter: Line feed (\n)
  First row as header: Selected
  Compression type: None

7. On the target step, in the Dataset substep, select the following settings:

  Target type: Azure Data Lake Storage Gen 2
  Connection: Select the existing connection to your data lake store (this was created for you when you created the workspace).

https://docs.microsoft.com/en-us/learn/modules/explore-fundamentals-stream-processing/2-batch-stream


























































===============
